{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10396146,"sourceType":"datasetVersion","datasetId":6441401},{"sourceId":10638793,"sourceType":"datasetVersion","datasetId":6587111},{"sourceId":10650002,"sourceType":"datasetVersion","datasetId":6594544},{"sourceId":10650008,"sourceType":"datasetVersion","datasetId":6594546},{"sourceId":10650013,"sourceType":"datasetVersion","datasetId":6594551}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Using distil multilingual BERT","metadata":{}},{"cell_type":"code","source":"print('MomoxOkarun')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:53:42.810421Z","iopub.execute_input":"2025-02-03T16:53:42.810746Z","iopub.status.idle":"2025-02-03T16:53:42.815121Z","shell.execute_reply.started":"2025-02-03T16:53:42.810716Z","shell.execute_reply":"2025-02-03T16:53:42.814277Z"}},"outputs":[{"name":"stdout","text":"MomoxOkarun\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install tensorboard","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T13:04:33.264277Z","iopub.execute_input":"2025-02-03T13:04:33.264557Z","iopub.status.idle":"2025-02-03T13:04:37.626965Z","shell.execute_reply.started":"2025-02-03T13:04:33.264536Z","shell.execute_reply":"2025-02-03T13:04:37.626145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    DistilBertForMaskedLM,\n    DistilBertTokenizerFast,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    EarlyStoppingCallback,\n    pipeline\n)\nfrom datasets import Dataset, load_dataset\nimport pandas as pd\nimport random\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:53:53.633980Z","iopub.execute_input":"2025-02-03T16:53:53.634366Z","iopub.status.idle":"2025-02-03T16:54:08.116363Z","shell.execute_reply.started":"2025-02-03T16:53:53.634334Z","shell.execute_reply":"2025-02-03T16:54:08.115743Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ========================\n# 1. Configuration\n# ========================\nSEED = 42\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nnp.random.seed(SEED)\nMODEL_NAME = \"distilbert-base-multilingual-cased\"\nwith open('/kaggle/input/finterms/financial_terms.txt') as f:\n    FINANCIAL_TERMS = f.read().splitlines()\n\nrandom.shuffle(FINANCIAL_TERMS)\nMAX_SEQ_LENGTH = 256\nTRAIN_RATIO = 0.85","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:58:37.316760Z","iopub.execute_input":"2025-02-03T16:58:37.317496Z","iopub.status.idle":"2025-02-03T16:58:37.359908Z","shell.execute_reply.started":"2025-02-03T16:58:37.317450Z","shell.execute_reply":"2025-02-03T16:58:37.359021Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ========================\n# 2. Helper Functions\n# ========================\ndef check_and_add_tokens(tokenizer, terms):\n    \"\"\"Identify terms needing addition and update tokenizer\"\"\"\n    terms_to_add = []\n    for term in terms:\n        tokens = tokenizer.tokenize(term)\n        if len(tokens) > 1 or not tokenizer.convert_tokens_to_ids(term):\n            terms_to_add.append(term)\n    \n    if terms_to_add:\n        print(f\"Adding {len(terms_to_add)} financial terms to tokenizer\")\n        tokenizer.add_tokens(terms_to_add)\n    \n    return tokenizer\n\ndef initialize_new_embeddings(model, tokenizer, new_terms):\n    \"\"\"Initialize new token embeddings with pre-trained averages\"\"\"\n    with torch.no_grad():\n        embeddings = model.get_input_embeddings().weight.data\n        new_embeddings = embeddings[:-len(new_terms)].mean(dim=0)\n        embeddings[-len(new_terms):] = new_embeddings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:58:39.196090Z","iopub.execute_input":"2025-02-03T16:58:39.196524Z","iopub.status.idle":"2025-02-03T16:58:39.202478Z","shell.execute_reply.started":"2025-02-03T16:58:39.196471Z","shell.execute_reply":"2025-02-03T16:58:39.201715Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ========================\n# 3. Data Preparation\n# ========================\n# Load and clean data\n\ngazetta_df = pd.read_csv(\"/kaggle/input/gazetta-financial-news-dataset/_--.csv\")\nrbk_df = pd.read_csv(\"/kaggle/input/rbk-financial-news-dataset/--.csv\")\nstockNews_df = pd.read_csv(\"/kaggle/input/stock-news-dataset/englishFinancialNews.csv\")\nprint(f\"Gazetta shape: {gazetta_df.shape}, RBK shape: {rbk_df.shape}, Stock News shape: {stockNews_df.shape}\\n\")\nprint(\"Gazetta info: \\n\", gazetta_df.info())\nprint(\"RBK info: \\n\", rbk_df.info())\nprint(\"Stock News info: \\n\", stockNews_df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:58:42.212083Z","iopub.execute_input":"2025-02-03T16:58:42.212714Z","iopub.status.idle":"2025-02-03T16:58:47.941157Z","shell.execute_reply.started":"2025-02-03T16:58:42.212678Z","shell.execute_reply":"2025-02-03T16:58:47.940453Z"}},"outputs":[{"name":"stdout","text":"Gazetta shape: (10937, 7), RBK shape: (16517, 6), Stock News shape: (11606, 5)\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10937 entries, 0 to 10936\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Unnamed: 0  10937 non-null  int64 \n 1   text        10937 non-null  object\n 2   summary     10937 non-null  object\n 3   title       10937 non-null  object\n 4   date        10937 non-null  object\n 5   url         10937 non-null  object\n 6   category    10937 non-null  object\ndtypes: int64(1), object(6)\nmemory usage: 598.2+ KB\nGazetta info: \n None\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 16517 entries, 0 to 16516\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Unnamed: 0  16517 non-null  int64 \n 1   url         16517 non-null  object\n 2   date        16517 non-null  object\n 3   title       16517 non-null  object\n 4   category    16517 non-null  object\n 5   text        16517 non-null  object\ndtypes: int64(1), object(5)\nmemory usage: 774.4+ KB\nRBK info: \n None\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 11606 entries, 0 to 11605\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   Unnamed: 0.1  11606 non-null  int64  \n 1   Unnamed: 0    11606 non-null  float64\n 2   title         11606 non-null  object \n 3   date          11605 non-null  object \n 4   stock         11605 non-null  object \ndtypes: float64(1), int64(1), object(3)\nmemory usage: 453.5+ KB\nStock News info: \n None\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"stockNews_df.rename(columns={\"title\":\"text\"}, inplace=True)\nprint(stockNews_df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:58:55.671950Z","iopub.execute_input":"2025-02-03T16:58:55.672262Z","iopub.status.idle":"2025-02-03T16:58:55.685022Z","shell.execute_reply.started":"2025-02-03T16:58:55.672238Z","shell.execute_reply":"2025-02-03T16:58:55.684006Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 11606 entries, 0 to 11605\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   Unnamed: 0.1  11606 non-null  int64  \n 1   Unnamed: 0    11606 non-null  float64\n 2   text          11606 non-null  object \n 3   date          11605 non-null  object \n 4   stock         11605 non-null  object \ndtypes: float64(1), int64(1), object(3)\nmemory usage: 453.5+ KB\nNone\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df1_text = gazetta_df[['text']].head(1000)\ndf2_text = rbk_df[['text']].head(5000)\ndf3_text = stockNews_df[['text']].head(3000)\n\n# Concatenate the DataFrames along the rows\ndf = pd.concat([df1_text, df2_text, df3_text], ignore_index=True)\ndf = df.sample(frac=1).reset_index(drop=True)\nprint(df.shape)\nprint(df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:02:26.014085Z","iopub.execute_input":"2025-02-03T17:02:26.014406Z","iopub.status.idle":"2025-02-03T17:02:26.032175Z","shell.execute_reply.started":"2025-02-03T17:02:26.014384Z","shell.execute_reply":"2025-02-03T17:02:26.031441Z"}},"outputs":[{"name":"stdout","text":"(9000, 1)\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9000 entries, 0 to 8999\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   text    9000 non-null   object\ndtypes: object(1)\nmemory usage: 70.4+ KB\nNone\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:02:29.413933Z","iopub.execute_input":"2025-02-03T17:02:29.414244Z","iopub.status.idle":"2025-02-03T17:02:29.424912Z","shell.execute_reply.started":"2025-02-03T17:02:29.414217Z","shell.execute_reply":"2025-02-03T17:02:29.424210Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                text\n0  Речь идет о развитии энергетического сотруднич...\n1  По итогам торгов в четверг, 6 ноября, доллар в...\n2  Live At 4 p.m. EDT, Benzinga Will Be Discussin...\n3  В четверг, 26 марта, на рынке европейских госу...\n4  В среду, 1 апреля, с открытием торгов на рынке...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Речь идет о развитии энергетического сотруднич...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>По итогам торгов в четверг, 6 ноября, доллар в...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Live At 4 p.m. EDT, Benzinga Will Be Discussin...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>В четверг, 26 марта, на рынке европейских госу...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>В среду, 1 апреля, с открытием торгов на рынке...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"dataset = Dataset.from_pandas(df[[\"text\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:02:34.438057Z","iopub.execute_input":"2025-02-03T17:02:34.438345Z","iopub.status.idle":"2025-02-03T17:02:34.810438Z","shell.execute_reply.started":"2025-02-03T17:02:34.438323Z","shell.execute_reply":"2025-02-03T17:02:34.809419Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ========================\n# 4. Tokenization Setup\n# ========================\ntokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n#tokenizer = check_and_add_tokens(tokenizer, FINANCIAL_TERMS)\n\ndef tokenize_fn(examples):\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=MAX_SEQ_LENGTH,\n        return_special_tokens_mask=True  # Helps data collator\n    )\n\ntokenized_ds = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\nsplit_ds = tokenized_ds.train_test_split(test_size=1-TRAIN_RATIO, seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:02:53.298820Z","iopub.execute_input":"2025-02-03T17:02:53.299131Z","iopub.status.idle":"2025-02-03T17:03:01.344068Z","shell.execute_reply.started":"2025-02-03T17:02:53.299107Z","shell.execute_reply":"2025-02-03T17:03:01.343236Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54720fc7a97b4aeeacc6686ae3d43819"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f9f3d61b1ad40ac83fa01dd61e1eac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd517e32597b45f58bfa178700e583bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"527cb1e6fc3c46e086071d69343e2e1b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"985dda13830c4bb2b1db52657b151f13"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"original_vocab_size = tokenizer.vocab_size  # ~120,000\n#new_vocab_size = original_vocab_size + len(FINANCIAL_TERMS)\n\n# Embedding layer shape before/after:\nprint(f\"Original: {original_vocab_size} x 768\")\n#print(f\"Updated: {new_vocab_size} x 768\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:03:01.345168Z","iopub.execute_input":"2025-02-03T17:03:01.345466Z","iopub.status.idle":"2025-02-03T17:03:01.350390Z","shell.execute_reply.started":"2025-02-03T17:03:01.345438Z","shell.execute_reply":"2025-02-03T17:03:01.349564Z"}},"outputs":[{"name":"stdout","text":"Original: 119547 x 768\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ========================\n# 5. Model Initialization\n# ========================\nmodel = DistilBertForMaskedLM.from_pretrained(MODEL_NAME)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Smart layer freezing: Unfreeze embeddings + last 3 layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze embeddings\nfor param in model.distilbert.embeddings.parameters():\n    param.requires_grad = True\n\n# #Unfreeze last 1 transformer layer\n# for layer in model.distilbert.transformer.layer[-1:]:\n#     for param in layer.parameters():\n#         param.requires_grad = True\n\n         \n# Define trainable components\ntrainable_components = [\n    \"transformer.layer.5\",  # Last transformer layer (layer 5)\n    \"vocab_transform\",       # MLM head components\n    \"vocab_layer_norm\",\n    \"vocab_projector.bias\"   # Only unfreeze the bias (weight may be tied)\n]\n\n# Unfreeze parameters matching the components\nfor name, param in model.named_parameters():\n    if any(component in name for component in trainable_components):\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\nfor name, param in model.named_parameters():\n    print(f\"{name} - Trainable: {param.requires_grad}\")\n\n# # Initialize new embeddings (if terms added)\n# if len(FINANCIAL_TERMS) > 0:\n#     initialize_new_embeddings(model, tokenizer, FINANCIAL_TERMS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:03:04.655786Z","iopub.execute_input":"2025-02-03T17:03:04.656110Z","iopub.status.idle":"2025-02-03T17:03:08.222698Z","shell.execute_reply.started":"2025-02-03T17:03:04.656081Z","shell.execute_reply":"2025-02-03T17:03:08.221791Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e30977dce242c0b5793a1bad2c4a40"}},"metadata":{}},{"name":"stdout","text":"distilbert.embeddings.word_embeddings.weight - Trainable: False\ndistilbert.embeddings.position_embeddings.weight - Trainable: False\ndistilbert.embeddings.LayerNorm.weight - Trainable: False\ndistilbert.embeddings.LayerNorm.bias - Trainable: False\ndistilbert.transformer.layer.0.attention.q_lin.weight - Trainable: False\ndistilbert.transformer.layer.0.attention.q_lin.bias - Trainable: False\ndistilbert.transformer.layer.0.attention.k_lin.weight - Trainable: False\ndistilbert.transformer.layer.0.attention.k_lin.bias - Trainable: False\ndistilbert.transformer.layer.0.attention.v_lin.weight - Trainable: False\ndistilbert.transformer.layer.0.attention.v_lin.bias - Trainable: False\ndistilbert.transformer.layer.0.attention.out_lin.weight - Trainable: False\ndistilbert.transformer.layer.0.attention.out_lin.bias - Trainable: False\ndistilbert.transformer.layer.0.sa_layer_norm.weight - Trainable: False\ndistilbert.transformer.layer.0.sa_layer_norm.bias - Trainable: False\ndistilbert.transformer.layer.0.ffn.lin1.weight - Trainable: False\ndistilbert.transformer.layer.0.ffn.lin1.bias - Trainable: False\ndistilbert.transformer.layer.0.ffn.lin2.weight - Trainable: False\ndistilbert.transformer.layer.0.ffn.lin2.bias - Trainable: False\ndistilbert.transformer.layer.0.output_layer_norm.weight - Trainable: False\ndistilbert.transformer.layer.0.output_layer_norm.bias - Trainable: False\ndistilbert.transformer.layer.1.attention.q_lin.weight - Trainable: False\ndistilbert.transformer.layer.1.attention.q_lin.bias - Trainable: False\ndistilbert.transformer.layer.1.attention.k_lin.weight - Trainable: False\ndistilbert.transformer.layer.1.attention.k_lin.bias - Trainable: False\ndistilbert.transformer.layer.1.attention.v_lin.weight - Trainable: False\ndistilbert.transformer.layer.1.attention.v_lin.bias - Trainable: False\ndistilbert.transformer.layer.1.attention.out_lin.weight - Trainable: False\ndistilbert.transformer.layer.1.attention.out_lin.bias - Trainable: False\ndistilbert.transformer.layer.1.sa_layer_norm.weight - Trainable: False\ndistilbert.transformer.layer.1.sa_layer_norm.bias - Trainable: False\ndistilbert.transformer.layer.1.ffn.lin1.weight - Trainable: False\ndistilbert.transformer.layer.1.ffn.lin1.bias - Trainable: False\ndistilbert.transformer.layer.1.ffn.lin2.weight - Trainable: False\ndistilbert.transformer.layer.1.ffn.lin2.bias - Trainable: False\ndistilbert.transformer.layer.1.output_layer_norm.weight - Trainable: False\ndistilbert.transformer.layer.1.output_layer_norm.bias - Trainable: False\ndistilbert.transformer.layer.2.attention.q_lin.weight - Trainable: False\ndistilbert.transformer.layer.2.attention.q_lin.bias - Trainable: False\ndistilbert.transformer.layer.2.attention.k_lin.weight - Trainable: False\ndistilbert.transformer.layer.2.attention.k_lin.bias - Trainable: False\ndistilbert.transformer.layer.2.attention.v_lin.weight - Trainable: False\ndistilbert.transformer.layer.2.attention.v_lin.bias - Trainable: False\ndistilbert.transformer.layer.2.attention.out_lin.weight - Trainable: False\ndistilbert.transformer.layer.2.attention.out_lin.bias - Trainable: False\ndistilbert.transformer.layer.2.sa_layer_norm.weight - Trainable: False\ndistilbert.transformer.layer.2.sa_layer_norm.bias - Trainable: False\ndistilbert.transformer.layer.2.ffn.lin1.weight - Trainable: False\ndistilbert.transformer.layer.2.ffn.lin1.bias - Trainable: False\ndistilbert.transformer.layer.2.ffn.lin2.weight - Trainable: False\ndistilbert.transformer.layer.2.ffn.lin2.bias - Trainable: False\ndistilbert.transformer.layer.2.output_layer_norm.weight - Trainable: False\ndistilbert.transformer.layer.2.output_layer_norm.bias - Trainable: False\ndistilbert.transformer.layer.3.attention.q_lin.weight - Trainable: False\ndistilbert.transformer.layer.3.attention.q_lin.bias - Trainable: False\ndistilbert.transformer.layer.3.attention.k_lin.weight - Trainable: False\ndistilbert.transformer.layer.3.attention.k_lin.bias - Trainable: False\ndistilbert.transformer.layer.3.attention.v_lin.weight - Trainable: False\ndistilbert.transformer.layer.3.attention.v_lin.bias - Trainable: False\ndistilbert.transformer.layer.3.attention.out_lin.weight - Trainable: False\ndistilbert.transformer.layer.3.attention.out_lin.bias - Trainable: False\ndistilbert.transformer.layer.3.sa_layer_norm.weight - Trainable: False\ndistilbert.transformer.layer.3.sa_layer_norm.bias - Trainable: False\ndistilbert.transformer.layer.3.ffn.lin1.weight - Trainable: False\ndistilbert.transformer.layer.3.ffn.lin1.bias - Trainable: False\ndistilbert.transformer.layer.3.ffn.lin2.weight - Trainable: False\ndistilbert.transformer.layer.3.ffn.lin2.bias - Trainable: False\ndistilbert.transformer.layer.3.output_layer_norm.weight - Trainable: False\ndistilbert.transformer.layer.3.output_layer_norm.bias - Trainable: False\ndistilbert.transformer.layer.4.attention.q_lin.weight - Trainable: False\ndistilbert.transformer.layer.4.attention.q_lin.bias - Trainable: False\ndistilbert.transformer.layer.4.attention.k_lin.weight - Trainable: False\ndistilbert.transformer.layer.4.attention.k_lin.bias - Trainable: False\ndistilbert.transformer.layer.4.attention.v_lin.weight - Trainable: False\ndistilbert.transformer.layer.4.attention.v_lin.bias - Trainable: False\ndistilbert.transformer.layer.4.attention.out_lin.weight - Trainable: False\ndistilbert.transformer.layer.4.attention.out_lin.bias - Trainable: False\ndistilbert.transformer.layer.4.sa_layer_norm.weight - Trainable: False\ndistilbert.transformer.layer.4.sa_layer_norm.bias - Trainable: False\ndistilbert.transformer.layer.4.ffn.lin1.weight - Trainable: False\ndistilbert.transformer.layer.4.ffn.lin1.bias - Trainable: False\ndistilbert.transformer.layer.4.ffn.lin2.weight - Trainable: False\ndistilbert.transformer.layer.4.ffn.lin2.bias - Trainable: False\ndistilbert.transformer.layer.4.output_layer_norm.weight - Trainable: False\ndistilbert.transformer.layer.4.output_layer_norm.bias - Trainable: False\ndistilbert.transformer.layer.5.attention.q_lin.weight - Trainable: True\ndistilbert.transformer.layer.5.attention.q_lin.bias - Trainable: True\ndistilbert.transformer.layer.5.attention.k_lin.weight - Trainable: True\ndistilbert.transformer.layer.5.attention.k_lin.bias - Trainable: True\ndistilbert.transformer.layer.5.attention.v_lin.weight - Trainable: True\ndistilbert.transformer.layer.5.attention.v_lin.bias - Trainable: True\ndistilbert.transformer.layer.5.attention.out_lin.weight - Trainable: True\ndistilbert.transformer.layer.5.attention.out_lin.bias - Trainable: True\ndistilbert.transformer.layer.5.sa_layer_norm.weight - Trainable: True\ndistilbert.transformer.layer.5.sa_layer_norm.bias - Trainable: True\ndistilbert.transformer.layer.5.ffn.lin1.weight - Trainable: True\ndistilbert.transformer.layer.5.ffn.lin1.bias - Trainable: True\ndistilbert.transformer.layer.5.ffn.lin2.weight - Trainable: True\ndistilbert.transformer.layer.5.ffn.lin2.bias - Trainable: True\ndistilbert.transformer.layer.5.output_layer_norm.weight - Trainable: True\ndistilbert.transformer.layer.5.output_layer_norm.bias - Trainable: True\nvocab_transform.weight - Trainable: True\nvocab_transform.bias - Trainable: True\nvocab_layer_norm.weight - Trainable: True\nvocab_layer_norm.bias - Trainable: True\nvocab_projector.bias - Trainable: True\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ========================\n# 6. Training Setup\n# ========================\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True,\n    mlm_probability=0.15,\n    pad_to_multiple_of=8  # Optimizes GPU utilization\n)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./finbert-mlm\",\n    logging_dir=\"./logs\",\n    num_train_epochs=5,\n    per_device_train_batch_size=16,  # Reduced from 64\n    per_device_eval_batch_size=16,\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=10,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    warmup_ratio=0.1,  # Better than fixed steps\n    lr_scheduler_type=\"inverse_sqrt\",\n    gradient_accumulation_steps=3,\n    fp16=True,\n    report_to=\"tensorboard\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    dataloader_num_workers=4,\n    dataloader_pin_memory=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=split_ds[\"train\"],\n    eval_dataset=split_ds[\"test\"],\n    data_collator=data_collator,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:07:14.683077Z","iopub.execute_input":"2025-02-03T17:07:14.683368Z","iopub.status.idle":"2025-02-03T17:07:14.725778Z","shell.execute_reply.started":"2025-02-03T17:07:14.683342Z","shell.execute_reply":"2025-02-03T17:07:14.725033Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ========================\n# 7. Baseline Model Evaluation\n# ========================\n\n# Initialize fresh model for baseline (don't freeze layers yet)\nbaseline_model = DistilBertForMaskedLM.from_pretrained(MODEL_NAME)\n#baseline_model.resize_token_embeddings(len(tokenizer))  # Match tokenizer changes\n\n# Evaluate baseline\ntrainer_baseline = Trainer(\n    model=baseline_model,\n    args=TrainingArguments(output_dir=\"./tmp\", report_to=\"none\"),\n    data_collator=data_collator,\n)\nbaseline_results = trainer_baseline.evaluate(eval_dataset=split_ds[\"test\"])\nbaseline_perplexity = torch.exp(torch.tensor(baseline_results[\"eval_loss\"])).item()\nprint(f\"Baseline Perplexity (Pre-Fine-Tuning): {baseline_perplexity:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:03:15.227743Z","iopub.execute_input":"2025-02-03T17:03:15.228063Z","iopub.status.idle":"2025-02-03T17:03:44.684771Z","shell.execute_reply.started":"2025-02-03T17:03:15.228035Z","shell.execute_reply":"2025-02-03T17:03:44.684048Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [85/85 00:26]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Baseline Perplexity (Pre-Fine-Tuning): 12.11\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# from transformers import pipeline\n# fill_mask = pipeline(\"fill-mask\", model=baseline_model, tokenizer=tokenizer)\n# print(fill_mask(\"Revenue grew by [MASK]% this quarter.\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T00:43:52.230335Z","iopub.execute_input":"2025-02-02T00:43:52.230683Z","iopub.status.idle":"2025-02-02T00:43:52.414562Z","shell.execute_reply.started":"2025-02-02T00:43:52.230655Z","shell.execute_reply":"2025-02-02T00:43:52.413445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fill_mask = pipeline(\"fill-mask\", model=baseline_model, tokenizer=tokenizer)\n# print(fill_mask(\"\\\"Юнипро\\\" [MASK] выработку электроэнергии в 1 полугодии на 17,1%\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T00:43:52.415592Z","iopub.execute_input":"2025-02-02T00:43:52.415988Z","iopub.status.idle":"2025-02-02T00:43:52.444896Z","shell.execute_reply.started":"2025-02-02T00:43:52.415953Z","shell.execute_reply":"2025-02-02T00:43:52.444020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================\n# 8. Training & Evaluation\n# ========================\ntrain_results = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:07:19.643278Z","iopub.execute_input":"2025-02-03T17:07:19.643649Z","iopub.status.idle":"2025-02-03T17:28:23.836234Z","shell.execute_reply.started":"2025-02-03T17:07:19.643620Z","shell.execute_reply":"2025-02-03T17:28:23.835225Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [400/400 21:00, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>2.068300</td>\n      <td>1.936039</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.041000</td>\n      <td>1.842458</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.975900</td>\n      <td>1.833567</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.951700</td>\n      <td>1.801491</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir ./logs --port 6006","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:29:14.626699Z","iopub.execute_input":"2025-02-03T17:29:14.627036Z","iopub.status.idle":"2025-02-03T17:29:20.170247Z","shell.execute_reply.started":"2025-02-03T17:29:14.627000Z","shell.execute_reply":"2025-02-03T17:29:20.169342Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"torch.cuda.memory_allocated() * (1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:29:29.033212Z","iopub.execute_input":"2025-02-03T17:29:29.033558Z","iopub.status.idle":"2025-02-03T17:29:29.038785Z","shell.execute_reply.started":"2025-02-03T17:29:29.033526Z","shell.execute_reply":"2025-02-03T17:29:29.038083Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"5513.914368"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:29:24.207772Z","iopub.execute_input":"2025-02-03T17:29:24.208081Z","iopub.status.idle":"2025-02-03T17:29:24.255241Z","shell.execute_reply.started":"2025-02-03T17:29:24.208056Z","shell.execute_reply":"2025-02-03T17:29:24.254365Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:29:27.335864Z","iopub.execute_input":"2025-02-03T17:29:27.336164Z","iopub.status.idle":"2025-02-03T17:29:27.340294Z","shell.execute_reply.started":"2025-02-03T17:29:27.336139Z","shell.execute_reply":"2025-02-03T17:29:27.339267Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Calculate final perplexity\neval_results = trainer.evaluate()\nperplexity = torch.exp(torch.tensor(eval_results[\"eval_loss\"])).item()\nprint(f\"Final Perplexity: {perplexity:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:29:31.489168Z","iopub.execute_input":"2025-02-03T17:29:31.489472Z","iopub.status.idle":"2025-02-03T17:29:57.011294Z","shell.execute_reply.started":"2025-02-03T17:29:31.489448Z","shell.execute_reply":"2025-02-03T17:29:57.010426Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='43' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [43/43 00:24]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final Perplexity: 6.04\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"train_results.metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:29:59.849361Z","iopub.execute_input":"2025-02-03T17:29:59.849759Z","iopub.status.idle":"2025-02-03T17:29:59.855088Z","shell.execute_reply.started":"2025-02-03T17:29:59.849719Z","shell.execute_reply":"2025-02-03T17:29:59.854274Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'train_runtime': 1263.8219,\n 'train_samples_per_second': 30.261,\n 'train_steps_per_second': 0.317,\n 'total_flos': 2540034945635616.0,\n 'train_loss': 2.0513448667526246,\n 'epoch': 5.0}"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"train_results.metrics[\"train_loss\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:30:03.065559Z","iopub.execute_input":"2025-02-03T17:30:03.065865Z","iopub.status.idle":"2025-02-03T17:30:03.070832Z","shell.execute_reply.started":"2025-02-03T17:30:03.065842Z","shell.execute_reply":"2025-02-03T17:30:03.069981Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"2.0513448667526246"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"trainer.state.log_history[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:30:04.649221Z","iopub.execute_input":"2025-02-03T17:30:04.649565Z","iopub.status.idle":"2025-02-03T17:30:04.655158Z","shell.execute_reply.started":"2025-02-03T17:30:04.649530Z","shell.execute_reply":"2025-02-03T17:30:04.654291Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"[{'loss': 2.3378,\n  'grad_norm': 147344.265625,\n  'learning_rate': 7.5e-06,\n  'epoch': 0.125,\n  'step': 10},\n {'loss': 2.279,\n  'grad_norm': 114982.6640625,\n  'learning_rate': 1.5e-05,\n  'epoch': 0.25,\n  'step': 20}]"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# ========================\n# 9. Monitoring (Optional)\n# ========================\n# Plot training loss\nif trainer.state.log_history:\n    train_loss = [log[\"loss\"] for log in trainer.state.log_history if \"loss\" in log]\n    plt.plot(train_loss)\n    plt.title(\"Training Loss Curve\")\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Loss\")\n    # Save the plot to a file\n    plt.savefig(\"/kaggle/working/imgs/training_loss_curve.png\")  # You can change the filename and format as needed\n    plt.close()  # Close the plot to free up memory\nelse:\n    print(\"No training loss data to plot!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:30:06.806996Z","iopub.execute_input":"2025-02-03T17:30:06.807289Z","iopub.status.idle":"2025-02-03T17:30:07.039678Z","shell.execute_reply.started":"2025-02-03T17:30:06.807265Z","shell.execute_reply":"2025-02-03T17:30:07.038793Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"if trainer.state.log_history:\n    train_loss = [log[\"eval_loss\"] for log in trainer.state.log_history if \"eval_loss\" in log]\n    plt.plot(train_loss)\n    plt.title(\"Evalutation Loss Curve\")\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Loss\")\n    plt.savefig(\"/kaggle/working/imgs/evalutation_loss_curve.png\")  # You can change the filename and format as needed\n    plt.close()  # Close the plot to free up memory\nelse:\n    print(\"No training loss data to plot!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:30:12.293965Z","iopub.execute_input":"2025-02-03T17:30:12.294259Z","iopub.status.idle":"2025-02-03T17:30:12.492537Z","shell.execute_reply.started":"2025-02-03T17:30:12.294236Z","shell.execute_reply":"2025-02-03T17:30:12.491282Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"if trainer.state.log_history:\n    train_loss = [log[\"grad_norm\"] for log in trainer.state.log_history if \"grad_norm\" in log]\n    plt.plot(train_loss)\n    plt.title(\"Gradient\")\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Gradient\")\n    plt.savefig(\"/kaggle/working/imgs/gradient_per_step_curve.png\")  # You can change the filename and format as needed\n    plt.close()  # Close the plot to free up memory\nelse:\n    print(\"No training loss data to plot!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:30:13.634378Z","iopub.execute_input":"2025-02-03T17:30:13.634731Z","iopub.status.idle":"2025-02-03T17:30:13.791673Z","shell.execute_reply.started":"2025-02-03T17:30:13.634701Z","shell.execute_reply":"2025-02-03T17:30:13.790731Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer, device=0)\nfill_mask_base = pipeline(\"fill-mask\", model=baseline_model, tokenizer=tokenizer, device=0)\nprint(fill_mask(\"Revenue grew by [MASK]% this quarter.\"), '\\n')\nprint(fill_mask_base(\"Revenue grew by [MASK]% this quarter.\"), '\\n')\nprint(fill_mask(\"\\\"Юнипро\\\" [MASK] выработку электроэнергии в 1 полугодии на 17,1%\"), '\\n')\nprint(fill_mask_base(\"\\\"Юнипро\\\" [MASK] выработку электроэнергии в 1 полугодии на 17,1%\"), '\\n')\nprint(fill_mask(\"Halliburton to [MASK] Several Contracts in Russia by Mid-May\"), '\\n')\nprint(fill_mask_base(\"Halliburton to [MASK] Several Contracts in Russia by Mid-May\"), '\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:34:06.194386Z","iopub.execute_input":"2025-02-03T17:34:06.194715Z","iopub.status.idle":"2025-02-03T17:34:06.277914Z","shell.execute_reply.started":"2025-02-03T17:34:06.194689Z","shell.execute_reply":"2025-02-03T17:34:06.277052Z"}},"outputs":[{"name":"stdout","text":"[{'score': 0.04261771962046623, 'token': 10150, 'token_str': '10', 'sequence': 'Revenue grew by 10 % this quarter.'}, {'score': 0.0355437733232975, 'token': 10197, 'token_str': '20', 'sequence': 'Revenue grew by 20 % this quarter.'}, {'score': 0.03483904153108597, 'token': 10709, 'token_str': '60', 'sequence': 'Revenue grew by 60 % this quarter.'}, {'score': 0.03136197105050087, 'token': 10208, 'token_str': '15', 'sequence': 'Revenue grew by 15 % this quarter.'}, {'score': 0.027442846447229385, 'token': 10244, 'token_str': '30', 'sequence': 'Revenue grew by 30 % this quarter.'}] \n\n[{'score': 0.03788019344210625, 'token': 10832, 'token_str': '80', 'sequence': 'Revenue grew by 80 % this quarter.'}, {'score': 0.03353461995720863, 'token': 11417, 'token_str': '75', 'sequence': 'Revenue grew by 75 % this quarter.'}, {'score': 0.03207072243094444, 'token': 10709, 'token_str': '60', 'sequence': 'Revenue grew by 60 % this quarter.'}, {'score': 0.03135935589671135, 'token': 11978, 'token_str': '95', 'sequence': 'Revenue grew by 95 % this quarter.'}, {'score': 0.02941235341131687, 'token': 10462, 'token_str': '50', 'sequence': 'Revenue grew by 50 % this quarter.'}] \n\n[{'score': 0.13083431124687195, 'token': 118, 'token_str': '-', 'sequence': '\" Юнипро \" - выработку электроэнергии в 1 полугодии на 17, 1 %'}, {'score': 0.07858347147703171, 'token': 96195, 'token_str': 'вы', 'sequence': '\" Юнипро \" вы выработку электроэнергии в 1 полугодии на 17, 1 %'}, {'score': 0.05504472926259041, 'token': 10234, 'token_str': 'за', 'sequence': '\" Юнипро \" за выработку электроэнергии в 1 полугодии на 17, 1 %'}, {'score': 0.05131378769874573, 'token': 117, 'token_str': ',', 'sequence': '\" Юнипро \", выработку электроэнергии в 1 полугодии на 17, 1 %'}, {'score': 0.038854219019412994, 'token': 10122, 'token_str': 'на', 'sequence': '\" Юнипро \" на выработку электроэнергии в 1 полугодии на 17, 1 %'}] \n\n[{'score': 0.0444953478872776, 'token': 21351, 'token_str': 'начала', 'sequence': '\" Юнипро \" начала выработку электроэнергии в 1 полугодии на 17, 1 %'}, {'score': 0.041218653321266174, 'token': 30860, 'token_str': 'получила', 'sequence': '\" Юнипро \" получила выработку электроэнергии в 1 полугодии на 17, 1 %'}, {'score': 0.041167739778757095, 'token': 17127, 'token_str': 'составляет', 'sequence': '\" Юнипро \" составляет выработку электроэнергии в 1 полугодии на 17, 1 %'}, {'score': 0.03936488553881645, 'token': 118, 'token_str': '-', 'sequence': '\" Юнипро \" - выработку электроэнергии в 1 полугодии на 17, 1 %'}, {'score': 0.03519890084862709, 'token': 23137, 'token_str': 'начал', 'sequence': '\" Юнипро \" начал выработку электроэнергии в 1 полугодии на 17, 1 %'}] \n\n[{'score': 0.07677490264177322, 'token': 13924, 'token_str': 'See', 'sequence': 'Halliburton to See Several Contracts in Russia by Mid - May'}, {'score': 0.040383342653512955, 'token': 14962, 'token_str': 'How', 'sequence': 'Halliburton to How Several Contracts in Russia by Mid - May'}, {'score': 0.0191452968865633, 'token': 95501, 'token_str': 'Launch', 'sequence': 'Halliburton to Launch Several Contracts in Russia by Mid - May'}, {'score': 0.01691349595785141, 'token': 14321, 'token_str': 'Be', 'sequence': 'Halliburton to Be Several Contracts in Russia by Mid - May'}, {'score': 0.016804704442620277, 'token': 24625, 'token_str': 'Say', 'sequence': 'Halliburton to Say Several Contracts in Russia by Mid - May'}] \n\n[{'score': 0.07694276422262192, 'token': 14962, 'token_str': 'How', 'sequence': 'Halliburton to How Several Contracts in Russia by Mid - May'}, {'score': 0.039375707507133484, 'token': 33623, 'token_str': 'establish', 'sequence': 'Halliburton to establish Several Contracts in Russia by Mid - May'}, {'score': 0.01756187155842781, 'token': 34307, 'token_str': 'operate', 'sequence': 'Halliburton to operate Several Contracts in Russia by Mid - May'}, {'score': 0.016467466950416565, 'token': 13924, 'token_str': 'See', 'sequence': 'Halliburton to See Several Contracts in Russia by Mid - May'}, {'score': 0.011944902129471302, 'token': 26756, 'token_str': 'reçoit', 'sequence': 'Halliburton to reçoit Several Contracts in Russia by Mid - May'}] \n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"print(tokenizer.tokenize(\"Юнипро увеличила выработку электроэнергии в 1 полугодии на 17,1%\"))\nprint(tokenizer.tokenize(\"«Газпром» сообщил о росте объема поставок в Китай на 60%\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:35:09.587012Z","iopub.execute_input":"2025-02-03T17:35:09.587327Z","iopub.status.idle":"2025-02-03T17:35:09.592757Z","shell.execute_reply.started":"2025-02-03T17:35:09.587305Z","shell.execute_reply":"2025-02-03T17:35:09.591971Z"}},"outputs":[{"name":"stdout","text":"['Ю', '##ни', '##про', 'у', '##вели', '##чила', 'вы', '##работку', 'электр', '##о', '##эн', '##ер', '##гии', 'в', '1', 'полу', '##годи', '##и', 'на', '17', ',', '1', '%']\n['«', 'Г', '##аз', '##про', '##м', '»', 'со', '##об', '##щил', 'о', 'рост', '##е', 'об', '##ъ', '##ема', 'поста', '##вок', 'в', 'Китай', 'на', '60', '%']\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"trainer.save_model(\"./finbert-mlm/finetuned_v2\")\ntokenizer.save_pretrained(\"./finbert-mlm/finetuned_v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:35:24.474602Z","iopub.execute_input":"2025-02-03T17:35:24.474933Z","iopub.status.idle":"2025-02-03T17:35:25.885352Z","shell.execute_reply.started":"2025-02-03T17:35:24.474893Z","shell.execute_reply":"2025-02-03T17:35:25.884351Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"('./finbert-mlm/finetuned_v2/tokenizer_config.json',\n './finbert-mlm/finetuned_v2/special_tokens_map.json',\n './finbert-mlm/finetuned_v2/vocab.txt',\n './finbert-mlm/finetuned_v2/added_tokens.json',\n './finbert-mlm/finetuned_v2/tokenizer.json')"},"metadata":{}}],"execution_count":39}]}